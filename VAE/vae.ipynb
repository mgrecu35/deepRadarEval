{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# define encoder in pytorch\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim,nl):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.nl=nl\n",
    "        self.conv1 = nn.Conv1d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 8, 3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(8, 8, 3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(8, 4, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(4*nl, latent_dim*2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        nl=self.nl\n",
    "        #print(x.shape)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        #print(x.size())\n",
    "        x = x.view(-1, 4*nl)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# define decoder in pytorch\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim,nl):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.nl=nl\n",
    "        self.fc1 = nn.Linear(latent_dim, nl*4)\n",
    "        self.conv1 = nn.ConvTranspose1d(4, 8, 3, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose1d(8, 8, 3, padding=1)\n",
    "        self.conv3 = nn.ConvTranspose1d(8, 16, 3, padding=1)\n",
    "        self.conv4 = nn.ConvTranspose1d(16, 3, 3, padding=1)\n",
    "    def forward(self, x):\n",
    "        nl=self.nl\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 4, self.nl)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        x = self.conv4(x)\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        return x\n",
    "\n",
    "# define variational autoencoder in pytorch\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim,nl):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim,nl)\n",
    "        self.decoder = Decoder(latent_dim,nl)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print('encoder')\n",
    "        x = self.encoder(x)\n",
    "        #print('endcoder done')\n",
    "        z_mean = x[:, 0:latent_dim]\n",
    "        z_log_var = x[:, latent_dim:]\n",
    "        #z = self.reparameterization(z_mean, z_log_var)\n",
    "        x = self.decoder(z_mean)\n",
    "        return x, z_mean, z_log_var\n",
    "\n",
    "    def reparameterization(self, z_mean, z_log_var):\n",
    "        std = torch.exp(z_log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(z_mean)\n",
    "\n",
    "latent_dim=7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2982523841.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    zL.append(dbz[-1,0,a[0][i]-1:a[0][i]+2,a[1][i]])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# Open the file cm1out.nc and read the variables\n",
    "ncfile = nc.Dataset('/Users/mgrecu/CM1/cm1r21.0/run/cm1out.nc')\n",
    "u = ncfile.variables['u'][:]\n",
    "v = ncfile.variables['v'][:]\n",
    "w = ncfile.variables['w'][:]\n",
    "dbz=ncfile.variables['dbz'][:]\n",
    "ncfile.close()\n",
    "a=np.nonzero(dbz[-1,0,:,:]>0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16782\n"
     ]
    }
   ],
   "source": [
    "zL=[]\n",
    "for it in range(-3,0):\n",
    "    a=np.nonzero(dbz[-1,0,:,:]>0)\n",
    "    for i in range(len(a[0])):\n",
    "        if a[1][i]>0 and a[1][i]<dbz.shape[3]-1 and a[0][i]>0 and a[0][i]<dbz.shape[2]-1:\n",
    "            zL.append(dbz[-1,:,a[0][i]-1:a[0][i]+2,a[1][i]])\n",
    "        \n",
    "\n",
    "#i=0\n",
    "print(len(zL))\n",
    "zL=np.array(zL)\n",
    "zL[zL<0]=0\n",
    "ss=zL.shape\n",
    "zL+=np.random.normal(0,0.5,ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# fit and transform in one step\n",
    "zL=zL[:,:64,:]\n",
    "z_scaled0 = scaler.fit_transform(zL[:,:,0])\n",
    "z_scaled1 = scaler.transform(zL[:,:,1])\n",
    "z_scaled2 = scaler.transform(zL[:,:,2])\n",
    "z_scaled=np.zeros((zL.shape[0],zL.shape[2],zL.shape[1]))\n",
    "z_scaled[:,0,:]=z_scaled0\n",
    "z_scaled[:,1,:]=z_scaled1\n",
    "z_scaled[:,2,:]=z_scaled2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16782, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(zL.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(z_scaled[:,0,:].mean(axis=0),z_scaled[:,0,:].std(axis=0))\n",
    "# import test train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split data into train and test sets\n",
    "train_x, test_x = train_test_split(z_scaled0, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder=Encoder(latent_dim,4)\n",
    "decoder=Decoder(latent_dim,4)\n",
    "vae=VAE(latent_dim,4)\n",
    "x=torch.randn(10,3,64)\n",
    "xp=vae(x)\n",
    "# train the the variational autoencoder model\n",
    "\n",
    "# include the KL divergence loss\n",
    "def loss_function(x_hat, x, z_mean, z_log_var):\n",
    "    recon_loss = criterion(x_hat, x)\n",
    "    #recon_loss=((x - x_hat)**2).sum()\n",
    "    \n",
    "    #kl_div = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
    "    #kl_div = ((z_log_var.exp())**2 + z_mean**2 - z_log_var - 1/2).sum()\n",
    "    return recon_loss #+ kl_div\n",
    "\n",
    "x_data = torch.randn(1000,3,64)\n",
    "print(x_data.dtype)\n",
    "# make a custom dataloader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "#print(xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderD(nn.Module):\n",
    "    def __init__(self, latent_dim, ninp, nh1, nh2):\n",
    "        super(EncoderD, self).__init__()\n",
    "        self.fc1 = nn.Linear(ninp, nh1)\n",
    "        self.fc2 = nn.Linear(nh1, nh2)\n",
    "        self.fc3 = nn.Linear(nh2, latent_dim*2)\n",
    "        self.nh1=nh1\n",
    "        self.nh2=nh2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# define decoder in pytorch\n",
    "class DecoderD(nn.Module):\n",
    "    def __init__(self, latent_dim, nh1, nh2, nout):\n",
    "        super(DecoderD, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, nh1)\n",
    "        self.fc2 = nn.Linear(nh1, nh2)\n",
    "        self.fc3 = nn.Linear(nh2, nout)\n",
    "        self.nh1=nh1\n",
    "        self.nh2=nh2\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# define variational autoencoder in pytorch\n",
    "class VAED(nn.Module):\n",
    "    def __init__(self, latent_dim,ninp,nh1,nh2,nout):\n",
    "        super(VAED, self).__init__()\n",
    "        self.encoder = EncoderD(latent_dim,ninp,nh1,nh2)\n",
    "        self.decoder = DecoderD(latent_dim,nh1,nh2,nout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z_mean = x[:, 0:latent_dim]\n",
    "        z_log_var = x[:, latent_dim:]\n",
    "        #z = self.reparameterization(z_mean, z_log_var)\n",
    "        x = self.decoder(z_mean)\n",
    "        return x, z_mean, z_log_var\n",
    "\n",
    "    def reparameterization(self, z_mean, z_log_var):\n",
    "        std = torch.exp(z_log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(z_mean)\n",
    "\n",
    "latent_dim=5\n",
    "ninp=50\n",
    "nout=50\n",
    "nh1=16\n",
    "nh2=16\n",
    "vae_model=VAED(latent_dim,ninp,nh1,nh2,nout)\n",
    "optimizer = optim.Adam(vae_model.parameters(), lr=1e-3)   \n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xd=train_x.astype(np.float32)\n",
    "print(type(train_xd))\n",
    "training_dataset = CustomDataset(train_xd[:,:50])\n",
    "train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "for epoch in range(30):\n",
    "    optimizer.zero_grad()\n",
    "    for x in iter(train_loader):\n",
    "    # sample randomnly a subset from x_data\n",
    "        #print(x.size(),x.dtype)\n",
    "        #print(x)\n",
    "        x_hat, z_mean, z_log_var = vae_model(x)\n",
    "        #print(x_hat.mean(axis=0))\n",
    "        loss = criterion(x_hat, x)\n",
    "        #break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, 100, loss.item()))\n",
    "    #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8602614239657664\n",
      "0.8643947197359401\n",
      "0.8699664663892035\n",
      "0.8767191931108047\n",
      "0.8789432545150696\n",
      "0.8937184438708254\n",
      "0.9003053167435612\n",
      "0.904084247891385\n",
      "0.8993322769979354\n",
      "0.902101997692287\n",
      "0.8952079678091176\n",
      "0.9021785745822102\n",
      "0.9068533981503365\n",
      "0.9150317236891765\n",
      "0.8860956770436951\n",
      "0.8281564393850359\n",
      "0.8592869288266074\n",
      "0.8560480768617976\n",
      "0.8477685795716313\n",
      "0.8404787634676419\n",
      "0.8373428611929232\n",
      "0.8546197609753172\n",
      "0.8628151535098803\n",
      "0.8543315205562543\n",
      "0.8429583547070253\n",
      "0.839762097170153\n",
      "0.8129037890019358\n",
      "0.8292906441885131\n",
      "0.844491475657399\n",
      "0.8412174301215888\n"
     ]
    }
   ],
   "source": [
    "x_hat, z_mean, z_log_var = vae_model(x)\n",
    "# get the numpy array from the tensor x_hat\n",
    "#x_hat = x_hat.detach().numpy()\n",
    "#print(np.corrcoef([x_hat[:,0,0],x.numpy()[:,0,0]]))\n",
    "#print(x_hat[:,0,0])\n",
    "#print(z_mean[:,0])\n",
    "for k in range(30):\n",
    "    print(np.corrcoef([x_hat.detach().numpy()[:,k],x.numpy()[:,k]])[0,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b36e6f780d74a6d4ea31a1262377f69b85f420f1a2aa7f634c439f3ee1fd7fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
