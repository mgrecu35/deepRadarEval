{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 16, 64])\n",
      "torch.Size([10, 4, 4])\n",
      "torch.Size([10, 16, 64])\n",
      "torch.Size([10, 4, 4])\n",
      "epoch [1/100], loss:0.9324\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# define encoder in pytorch\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim,nl):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.nl=nl\n",
    "        self.conv1 = nn.Conv1d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 8, 3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(8, 8, 3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(8, 4, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(4*nl, latent_dim*2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        nl=self.nl\n",
    "        print(x.shape)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        print(x.size())\n",
    "        x = x.view(-1, 4*nl)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# define decoder in pytorch\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim,nl):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.nl=nl\n",
    "        self.fc1 = nn.Linear(latent_dim, nl*4)\n",
    "        self.conv1 = nn.ConvTranspose1d(4, 8, 3, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose1d(8, 8, 3, padding=1)\n",
    "        self.conv3 = nn.ConvTranspose1d(8, 16, 3, padding=1)\n",
    "        self.conv4 = nn.ConvTranspose1d(16, 3, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        nl=self.nl\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 4, self.nl)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        return x\n",
    "\n",
    "# define variational autoencoder in pytorch\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim,nl):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim,nl)\n",
    "        self.decoder = Decoder(latent_dim,nl)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z_mean = x[:, 0:latent_dim]\n",
    "        z_log_var = x[:, latent_dim:]\n",
    "        z = self.reparameterization(z_mean, z_log_var)\n",
    "        x = self.decoder(z)\n",
    "        return x, z_mean, z_log_var\n",
    "\n",
    "    def reparameterization(self, z_mean, z_log_var):\n",
    "        std = torch.exp(0.5 * z_log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(z_mean)\n",
    "\n",
    "latent_dim=5\n",
    "\n",
    "encoder=Encoder(latent_dim,4)\n",
    "decoder=Decoder(latent_dim,4)\n",
    "vae=VAE(latent_dim,4)\n",
    "x=torch.randn(10,3,64)\n",
    "xp=vae(x)\n",
    "# train the the variational autoencoder model\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)   \n",
    "criterion = nn.MSELoss()\n",
    "# include the KL divergence loss\n",
    "def loss_function(x_hat, x, z_mean, z_log_var):\n",
    "    recon_loss = criterion(x_hat, x)\n",
    "    kl_div = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "x_data = torch.randn(1000,3,64)\n",
    "# make a custom dataloader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "training_dataset = CustomDataset(x_data)\n",
    "train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "for epoch in range(1):\n",
    "    optimizer.zero_grad()\n",
    "    x = next(iter(train_loader))\n",
    "    # sample randomnly a subset from x_data\n",
    "    x_hat, z_mean, z_log_var = vae(x)\n",
    "    loss = criterion(x_hat, x)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, 100, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(x.size(1))\n",
    "print(len(iter(train_loader)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b36e6f780d74a6d4ea31a1262377f69b85f420f1a2aa7f634c439f3ee1fd7fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
